# Copyright (c) 2025-2026 SandAI. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from typing import Any, overload

import torch
import torch.distributed as dist

from magi_attention.common.enum import GroupReduceOp
from magi_attention.utils import nvtx, wrap_to_list

from ...work import GeneralWork, WorkWithPostProcessFn
from ._buffer import GrpCollBuffer
from ._config import GrpCollConfig
from ._handle import GrpCollHandle
from ._mgr import grpcoll_mgr
from .utils import (
    get_a2av_perm_idxs_from_group_cast_meta,
    get_group_reduce_handle_from_sym_group_cast,
    get_native_group_cast_meta,
    maybe_lazy_init_buffer,
)

__all__ = [
    "native_group_cast_impl",
    "native_group_reduce_impl",
]


# ------------------        native group cast       ------------------ #


# host meta interface
@overload
def native_group_cast_impl(
    input: torch.Tensor,
    output: torch.Tensor | None,
    input_split_sizes: list[int],
    output_split_sizes: list[int],
    dst_indices: list[list[int]],
    src_index: list[int],
    group: dist.ProcessGroup,
    async_op: bool = False,
    cast_lse: bool = False,
    input_lse: torch.Tensor | None = None,
    output_lse: torch.Tensor | None = None,
    **kwargs,
) -> WorkWithPostProcessFn:
    ...


# device meta interface
@overload
def native_group_cast_impl(
    input: torch.Tensor,
    output: torch.Tensor | None,
    input_split_sizes: torch.Tensor,
    output_split_sizes: torch.Tensor,
    dst_indices: torch.Tensor,
    src_index: torch.Tensor,
    group: dist.ProcessGroup,
    async_op: bool = False,
    cast_lse: bool = False,
    input_lse: torch.Tensor | None = None,
    output_lse: torch.Tensor | None = None,
    **kwargs,
) -> WorkWithPostProcessFn:
    ...


@nvtx.instrument_nvtx
def native_group_cast_impl(
    input: torch.Tensor,
    output: torch.Tensor | None,
    input_split_sizes: list[int] | torch.Tensor,
    output_split_sizes: list[int] | torch.Tensor,
    dst_indices: list[list[int]] | torch.Tensor,
    src_index: list[int] | torch.Tensor,
    group: dist.ProcessGroup,
    async_op: bool = False,
    cast_lse: bool = False,
    input_lse: torch.Tensor | None = None,
    output_lse: torch.Tensor | None = None,
    **kwargs,
) -> WorkWithPostProcessFn:
    """Native group-cast implementation"""
    # maybe lazy init buffer
    maybe_lazy_init_buffer(group)

    # get grpcoll config and buffer
    config: GrpCollConfig = grpcoll_mgr.get_config(group)
    buffer: GrpCollBuffer = grpcoll_mgr.get_buffer(group)
    assert config is not None and buffer is not None

    # pack input and output
    input: list[torch.Tensor] = wrap_to_list(input)
    output: list[torch.Tensor] | None = (
        wrap_to_list(output) if output is not None else output
    )
    num_groups = len(input)

    # get seqlen info
    input_seqlen: int = input[0].size(0)
    output_seqlen: int | None = (
        output[0].size(0) if output is not None else kwargs.pop("output_seqlen", None)
    )
    internode_output_seqlen: int = kwargs.pop("internode_output_seqlen", -1)

    # get meta dict and handle
    meta_dict: dict[str, Any] = kwargs.pop("native_group_cast_meta_dict", {})
    handle_dict: dict[str, GrpCollHandle] = kwargs.pop("native_grpcoll_handle_dict", {})
    handle: GrpCollHandle | None = handle_dict.get("group_cast", None)

    # transfer to native group-cast meta args
    if meta_dict:
        num_tokens_per_rank = meta_dict["num_tokens_per_rank"]
        num_tokens_per_rdma_rank = meta_dict["num_tokens_per_rdma_rank"]
        is_token_in_rank = meta_dict["is_token_in_rank"]
        post_perm_idx = meta_dict["post_perm_idx"]
    else:
        (
            num_tokens_per_rank,
            num_tokens_per_rdma_rank,
            is_token_in_rank,
        ) = get_native_group_cast_meta(
            input_split_sizes=input_split_sizes,
            dst_indices=dst_indices,
            group=group,
            input_seqlen=input_seqlen,
            # HACK: leave a slot for t2r_idx
            # since for now, we transfer the group_cast meta to it inside anyway
            # which is helpful in the token-level communication scenarios such as ep, nsa
            t2r_idx=kwargs.pop("t2r_idx", None),
        )

        # for group-cast, perm_to_a2av_idx is the post_perm_idx
        post_perm_idx = get_a2av_perm_idxs_from_group_cast_meta(
            output_split_sizes=output_split_sizes,
            src_index=src_index,
            num_ranks=group.size(),
            output_seqlen=output_seqlen,
        )

    # launch group cast kernel
    (
        recv_x,
        recv_lse,
        handle,
        event,
    ) = buffer.group_cast(
        x=input,
        recv_x=output,
        handle=handle,
        num_tokens_per_rank=num_tokens_per_rank,
        num_tokens_per_rdma_rank=num_tokens_per_rdma_rank,
        is_token_in_rank=is_token_in_rank,
        post_perm_idx=post_perm_idx,
        config=config,
        previous_event=None,
        async_op=async_op,
        allocate_on_comm_stream=False,
        cast_lse=cast_lse,
        lse=input_lse,
        recv_lse=output_lse,
        max_num_rdma_recv_tokens=internode_output_seqlen,
    )

    # unpack recv_x
    if num_groups == 1:
        recv_x = recv_x[0]

    # HACK: prepare handle for symmetric group-reduce or cached group-cast
    handle_dict["group_cast"] = handle
    handle_dict["group_reduce"] = handle

    # prepare work with post-process
    work_with_post_process_fn = WorkWithPostProcessFn(
        work=GeneralWork(event),
        post_process_fn=(
            (lambda *args, **kwargs: (recv_x, recv_lse))
            if cast_lse
            else lambda *args, **kwargs: recv_x
        ),
        async_op=async_op,
    )

    return work_with_post_process_fn


# ------------------        native group reduce       ------------------ #


# host meta interface
@overload
def native_group_reduce_impl(
    input: torch.Tensor,
    output: torch.Tensor | None,
    input_split_sizes: list[int],
    output_split_sizes: list[int],
    dst_index: list[int],
    src_indices: list[list[int]],
    group: dist.ProcessGroup,
    async_op: bool = False,
    reduce_op: GroupReduceOp = "sum",
    acc_reduce: bool = True,
    comm_dtype: torch.dtype | None = None,
    input_lse: torch.Tensor | None = None,
    output_lse: torch.Tensor | None = None,
    **kwargs,
) -> WorkWithPostProcessFn:
    ...


# device meta interface
@overload
def native_group_reduce_impl(
    input: torch.Tensor,
    output: torch.Tensor | None,
    input_split_sizes: torch.Tensor,
    output_split_sizes: torch.Tensor,
    dst_index: torch.Tensor,
    src_indices: torch.Tensor,
    group: dist.ProcessGroup,
    async_op: bool = False,
    reduce_op: GroupReduceOp = "sum",
    acc_reduce: bool = True,
    comm_dtype: torch.dtype | None = None,
    input_lse: torch.Tensor | None = None,
    output_lse: torch.Tensor | None = None,
    **kwargs,
) -> WorkWithPostProcessFn:
    ...


@nvtx.instrument_nvtx
def native_group_reduce_impl(
    input: torch.Tensor,
    output: torch.Tensor | None,
    input_split_sizes: list[int] | torch.Tensor,
    output_split_sizes: list[int] | torch.Tensor,
    dst_index: list[int] | torch.Tensor,
    src_indices: list[list[int]] | torch.Tensor,
    group: dist.ProcessGroup,
    async_op: bool = False,
    reduce_op: GroupReduceOp = "sum",
    acc_reduce: bool = True,
    comm_dtype: torch.dtype | None = None,
    input_lse: torch.Tensor | None = None,
    output_lse: torch.Tensor | None = None,
    **kwargs,
) -> WorkWithPostProcessFn:
    """Native group-reduce implementation"""
    # maybe lazy init buffer
    maybe_lazy_init_buffer(group)

    # get grpcoll config and buffer
    config: GrpCollConfig = grpcoll_mgr.get_config(group)
    buffer: GrpCollBuffer = grpcoll_mgr.get_buffer(group)
    assert config is not None and buffer is not None

    # pack input and output
    input: list[torch.Tensor] = wrap_to_list(input)
    output: list[torch.Tensor] | None = (
        wrap_to_list(output) if output is not None else output
    )
    num_groups = len(input)

    # get seqlen info
    input_seqlen: int = input[0].size(0)
    output_seqlen: int | None = (
        output[0].size(0) if output is not None else kwargs.pop("output_seqlen", None)
    )

    # get meta dict and handle
    meta_dict: dict[str, Any] = kwargs.pop("native_group_reduce_meta_dict", {})
    handle_dict: dict[str, GrpCollHandle] = kwargs.pop("native_grpcoll_handle_dict", {})
    handle: GrpCollHandle | None = handle_dict.get("group_reduce", None)
    if handle is None:
        # FIXME: for now, we don't support individual group-reduce
        # since the necessary handle is not known until the symmetric group-cast returns
        handle = get_group_reduce_handle_from_sym_group_cast(
            input=input[0],
            output=output[0] if output is not None else None,
            input_split_sizes=input_split_sizes,
            output_split_sizes=output_split_sizes,
            dst_index=dst_index,
            src_indices=src_indices,
            group=group,
            async_op=async_op,
            output_seqlen=output_seqlen,
            t2r_idx=kwargs.pop("t2r_idx", None),
        )

    # transfer to symmetric native group-cast meta args
    if meta_dict:
        pre_perm_idx = meta_dict["pre_perm_idx"]
    else:
        # for group-reduce, perm_to_a2av_idx is the pre_perm_idx
        # the same as the post_perm_idx for symmetric group-cast
        pre_perm_idx = get_a2av_perm_idxs_from_group_cast_meta(
            output_split_sizes=input_split_sizes,
            src_index=dst_index,
            num_ranks=group.size(),
            output_seqlen=input_seqlen,
        )

    # launch group reduce kernel
    (
        reduced_x,
        reduced_lse,
        event,
    ) = buffer.group_reduce(
        x=input,
        handle=handle,
        reduced_x=output,
        reduce_op=reduce_op,
        acc_reduce=acc_reduce,
        pre_perm_idx=pre_perm_idx,
        config=config,
        previous_event=None,
        async_op=async_op,
        allocate_on_comm_stream=False,
        comm_dtype=comm_dtype,
        lse=input_lse,
        reduced_lse=output_lse,
    )

    # unpack reduced_x
    if num_groups == 1:
        reduced_x = reduced_x[0]

    # HACK: prepare handle for symmetric group-cast or cached group-reduce
    # REVIEW: should we empty the handle dict since the tensors in handle is inplace modified ?
    handle_dict["group_cast"] = handle
    handle_dict["group_reduce"] = handle

    # prepare work with post-process
    work_with_post_process_fn = WorkWithPostProcessFn(
        work=GeneralWork(event),
        post_process_fn=(
            (lambda *args, **kwargs: (reduced_x, reduced_lse))
            if reduce_op == "lse"
            else lambda *args, **kwargs: reduced_x
        ),
        async_op=async_op,
    )

    return work_with_post_process_fn
