/**********************************************************************************
 * Copyright (c) 2025-2026 SandAI. All Rights Reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *********************************************************************************/

/******************************************************************************
 * Copyright (c) 2024, Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, Tri Dao.
 ******************************************************************************/

#pragma once

#include <cute/tensor.hpp>
#include <cutlass/barrier.h>
#include <cutlass/cutlass.h>

#include "cutlass/gemm/collective/builders/sm90_common.inl"

#include "named_barrier.hpp"
#include "seqlen.h"
#include "utils.h"

namespace flash {

using namespace cute;
namespace gcd = cutlass::gemm::collective::detail;

template <
    class TileShape_MNK_,
    class Element_,
    class ElementAccum_,
    class ArchTag_,
    class BlockCoordType_,
    bool dQ_swapAB_,
    bool dKV_swapAB_,
    int NumMmaWarpGroups = 2,
    int AtomLayoutMdQ = 1,
    int AtomLayoutNdKV = 2,
    bool DisableBwdDkvAtomicReduction_ = false,
    bool Deterministic_ = false,
    bool SwapBwdQKLoop_ = false>
struct CollectiveEpilogueBwd {
  using TileShape_MNK = TileShape_MNK_;
  using Element = Element_;
  using ElementAccum = ElementAccum_;
  using ArchTag = ArchTag_;
  using BlockCoordType = BlockCoordType_;
  using SeqlenInfo_t = flash::DistributedSeqlenInfo;
  using resv_barrier = cutlass::arch::ReservedNamedBarriers;

  // Sanity check
  static_assert(ArchTag::kMinComputeCapability >= 90);

  static constexpr bool IsSameType = cute::is_same_v<Element, ElementAccum>;
  static constexpr bool DisableBwdDkvAtomicReduction = DisableBwdDkvAtomicReduction_;

  static constexpr bool dQ_swapAB = dQ_swapAB_;
  static constexpr bool dKV_swapAB = dKV_swapAB_;
  static constexpr bool Use_TMA = ArchTag::kMinComputeCapability >= 90;
  static constexpr bool Deterministic = Deterministic_;
  static constexpr bool SwapBwdQKLoop = SwapBwdQKLoop_;

  static constexpr int NumEpilogueThreads = NumMmaWarpGroups * cutlass::NumThreadsPerWarpGroup;
  static constexpr int AtomLayoutMdKV = NumMmaWarpGroups * (Use_TMA ? 1 : cutlass::NumWarpsPerWarpGroup) / AtomLayoutNdKV;
  static constexpr int AtomLayoutNdQ = NumMmaWarpGroups * (Use_TMA ? 1 : cutlass::NumWarpsPerWarpGroup) / AtomLayoutMdQ;

  static constexpr int kBlockM = get<0>(TileShape_MNK{});
  static constexpr int kBlockN = get<1>(TileShape_MNK{});
  static constexpr int kHeadDim = get<2>(TileShape_MNK{});

  using GmemTiledCopydQTMA = cute::SM90_TMA_REDUCE_ADD; // TODO: maybe also add DisableBwdDqAtomicReduction flag ?
  using GmemTiledCopydKVTMA = std::conditional_t<DisableBwdDkvAtomicReduction, cute::SM90_TMA_STORE, cute::SM90_TMA_REDUCE_ADD>;
  using BwdNamedBarriers = std::conditional_t<SwapBwdQKLoop, BwdNamedBarriersLoopK, BwdNamedBarriersLoopQ>;
  static_assert(BarrierManager::check<BwdNamedBarriers, NumMmaWarpGroups>());

  // These are for storing the output tensor without TMA (e.g., for setting output to zero)
  static constexpr int kGmemElemsPerLoad = sizeof(cute::uint128_t) / sizeof(Element);
  static_assert(kHeadDim % kGmemElemsPerLoad == 0, "Headdim must be a multiple of kGmemElemsPerLoad");
  static constexpr int kGmemThreadsPerRow = cutlass::gcd(kHeadDim / kGmemElemsPerLoad, NumEpilogueThreads);
  static_assert(NumEpilogueThreads % kGmemThreadsPerRow == 0, "NumEpilogueThreads must be a multiple of kGmemThreadsPerRow");

  using GmemLayoutAtom = Layout<Shape<Int<NumEpilogueThreads / kGmemThreadsPerRow>, Int<kGmemThreadsPerRow>>, Stride<Int<kGmemThreadsPerRow>, _1>>;
  using GmemTiledCopydQ = decltype(make_tiled_copy(
      Copy_Atom<AutoVectorizingCopyWithAssumedAlignment<128>, Element>{},
      GmemLayoutAtom{},
      Layout<Shape<_1, Int<kGmemElemsPerLoad>>>{})); // Val layout, 8 or 16 vals per store

  using GmemTiledCopydKV = decltype(make_tiled_copy(
      Copy_Atom<AutoVectorizingCopyWithAssumedAlignment<128>, Element>{},
      GmemLayoutAtom{},
      Layout<Shape<_1, Int<kGmemElemsPerLoad>>>{})); // Val layout, 8 or 16 vals per store

  using SmemLayoutAtomdQTMA = decltype(gcd::ss_smem_selector<
                                       GMMA::Major::K,
                                       Element,
                                       // TODO: do we have to change this if dQ_swapAB is true?
                                       Int<kBlockM>,
                                       Int<kHeadDim / AtomLayoutNdQ>>());
  using SmemLayoutdQTMA = decltype(tile_to_shape(SmemLayoutAtomdQTMA{}, select<0, 2>(TileShape_MNK{})));
  using SmemLayoutdQtTMA = decltype(cute::composition(SmemLayoutdQTMA{}, make_layout(make_shape(Int<kHeadDim>{}, Int<kBlockM>{}), make_stride(Int<kBlockM>{}, _1{}))));

  using SmemLayoutAtomdKVTMA = decltype(gcd::ss_smem_selector<
                                        GMMA::Major::K,
                                        Element,
                                        // TODO: do we have to change this if dKV_swapAB is true?
                                        Int<kBlockN>,
                                        Int<kHeadDim / AtomLayoutMdKV>>());
  using SmemLayoutdKVTMA = decltype(tile_to_shape(SmemLayoutAtomdKVTMA{}, select<1, 2>(TileShape_MNK{})));
  using SmemLayoutdKVtTMA =
      decltype(cute::composition(SmemLayoutdKVTMA{}, make_layout(make_shape(Int<kHeadDim>{}, Int<kBlockN>{}), make_stride(Int<kBlockN>{}, _1{}))));

  using SmemLayoutAtomdQ = SmemLayoutAtomdQTMA;
  using SmemLayoutdQ = decltype(tile_to_shape(SmemLayoutAtomdQ{}, select<0, 2>(TileShape_MNK{})));
  using SmemLayoutdQt = decltype(cute::composition(SmemLayoutdQ{}, make_layout(make_shape(Int<kHeadDim>{}, Int<kBlockM>{}), make_stride(Int<kBlockM>{}, _1{}))));

  using SmemLayoutAtomdKV = SmemLayoutAtomdKVTMA;
  using SmemLayoutdKV = decltype(tile_to_shape(SmemLayoutAtomdKV{}, select<1, 2>(TileShape_MNK{})));
  using SmemLayoutdKVt = decltype(cute::composition(SmemLayoutdKV{}, make_layout(make_shape(Int<kHeadDim>{}, Int<kBlockN>{}), make_stride(Int<kBlockN>{}, _1{}))));

  using SmemCopyAtomdQ = Copy_Atom<cute::DefaultCopy, Element>;
  using SmemCopyAtomdKV = Copy_Atom<cute::DefaultCopy, Element>;

  static constexpr size_t SmemAlignmentdQ = ArchTag::kMinComputeCapability >= 90 ? cutlass::detail::alignment_for_swizzle(SmemLayoutdQ{}) : 128;
  static_assert(SmemAlignmentdQ >= 128, "Require at least 128B alignment");

  static constexpr size_t SmemAlignmentdKV = ArchTag::kMinComputeCapability >= 90 ? cutlass::detail::alignment_for_swizzle(SmemLayoutdKV{}) : 128;
  static_assert(SmemAlignmentdKV >= 128, "Require at least 128B alignment");

  struct TensorStorageLoopQ : cute::aligned_struct<SmemAlignmentdKV> {
    cute::array_aligned<Element, cute::cosize_v<SmemLayoutdKV>, SmemAlignmentdKV> smem_dk;
    cute::array_aligned<Element, cute::cosize_v<SmemLayoutdKV>, SmemAlignmentdKV> smem_dv;
  };

  struct TensorStorageLoopK : cute::aligned_struct<SmemAlignmentdQ> {
    cute::array_aligned<Element, cute::cosize_v<SmemLayoutdQ>, SmemAlignmentdQ> smem_dq;
  };

  using TensorStorage = std::conditional_t<SwapBwdQKLoop, TensorStorageLoopK, TensorStorageLoopQ>;

  using ShapedQKV = cute::Shape<int32_t, int32_t, int32_t>; // (seqlen, head_dim, num_heads)
  using StridedQKV = cute::Stride<int64_t, _1, int64_t>;

  using TMA_dQ = std::conditional_t<
      Use_TMA,
      decltype(make_tma_copy(
          GmemTiledCopydQTMA{},
          make_tensor(make_gmem_ptr(static_cast<Element*>(nullptr)), ShapedQKV{}, StridedQKV{}),
          SmemLayoutdQTMA{},
          select<0, 2>(TileShape_MNK{}),
          _1{})), // no mcast for dQ
      std::nullptr_t>;

  using TMA_dKV = std::conditional_t<
      Use_TMA,
      decltype(make_tma_copy(
          GmemTiledCopydKVTMA{},
          make_tensor(make_gmem_ptr(static_cast<Element*>(nullptr)), ShapedQKV{}, StridedQKV{}),
          SmemLayoutdKVTMA{},
          select<1, 2>(TileShape_MNK{}),
          _1{})), // no mcast for dKV
      std::nullptr_t>;

  // Host side kernel arguments
  struct Arguments {
    Element* ptr_dQ; // q for outer-loop and k for inner-loop
    ShapedQKV const shape_dQ;
    StridedQKV const stride_dQ;
    Element* ptr_dK; // k for outer-loop and q for inner-loop
    ShapedQKV const shape_dK;
    StridedQKV const stride_dK;
    Element* ptr_dV; // k for outer-loop and q for inner-loop
    ShapedQKV const shape_dV;
    StridedQKV const stride_dV;
    int const num_heads_q;
    int const num_heads_kv;
    int2 const* q_ranges;
    int2 const* k_ranges;
    int* determin_range_locks = nullptr;
  };

  // Device side kernel params
  struct Params {
    Element* ptr_dQ; // q for outer-loop and k for inner-loop
    ShapedQKV const shape_dQ;
    StridedQKV const stride_dQ;
    Element* ptr_dK; // k for outer-loop and q for inner-loop
    ShapedQKV const shape_dK;
    StridedQKV const stride_dK;
    Element* ptr_dV; // k for outer-loop and q for inner-loop
    ShapedQKV const shape_dV;
    StridedQKV const stride_dV;
    TMA_dQ tma_store_dQ; // q for outer-loop and k for inner-loop
    TMA_dKV tma_store_dK; // k for outer-loop and q for inner-loop
    TMA_dKV tma_store_dV; // k for outer-loop and q for inner-loop
    int2 const* q_ranges;
    int2 const* k_ranges;
    cutlass::FastDivmod qhead_per_khead_divmod;
    int const nheads;
    int* determin_range_locks = nullptr;
  };

  static Params to_underlying_arguments(Arguments const& args) {
    Tensor mdQ = make_tensor(make_gmem_ptr(args.ptr_dQ), args.shape_dQ, args.stride_dQ);
    Tensor mdK = make_tensor(make_gmem_ptr(args.ptr_dK), args.shape_dK, args.stride_dK);
    Tensor mdV = make_tensor(make_gmem_ptr(args.ptr_dV), args.shape_dV, args.stride_dV);

    TMA_dQ tma_store_dQ = [&] {
      if constexpr (Use_TMA) {
        return make_tma_copy(GmemTiledCopydQTMA{}, mdQ, SmemLayoutdQTMA{}, select<0, 2>(TileShape_MNK{}), _1{});
      } else {
        return nullptr;
      }
    }();
    TMA_dKV tma_store_dK = [&] {
      if constexpr (Use_TMA) {
        return make_tma_copy(GmemTiledCopydKVTMA{}, mdK, SmemLayoutdKVTMA{}, select<1, 2>(TileShape_MNK{}), _1{});
      } else {
        return nullptr;
      }
    }();
    TMA_dKV tma_store_dV = [&] {
      if constexpr (Use_TMA) {
        return make_tma_copy(GmemTiledCopydKVTMA{}, mdV, SmemLayoutdKVTMA{}, select<1, 2>(TileShape_MNK{}), _1{});
      } else {
        return nullptr;
      }
    }();
    return {
        args.ptr_dQ,
        args.shape_dQ,
        args.stride_dQ,
        args.ptr_dK,
        args.shape_dK,
        args.stride_dK,
        args.ptr_dV,
        args.shape_dV,
        args.stride_dV,
        tma_store_dQ,
        tma_store_dK,
        tma_store_dV,
        args.q_ranges,
        args.k_ranges,
        /*qhead_per_khead_divmod=*/cutlass::FastDivmod(cute::ceil_div(args.num_heads_q, get<2>(args.shape_dK))),
        args.num_heads_kv,
        args.determin_range_locks};
  }

  // Issue Tma Descriptor Prefetch -- ideally from a single thread for best performance
  CUTLASS_DEVICE
  static void prefetch_tma_descriptors(Params const& params) {
    if constexpr (Use_TMA) {
      if constexpr (SwapBwdQKLoop) {
        cute::prefetch_tma_descriptor(params.tma_store_dQ.get_tma_descriptor());
      } else {
        cute::prefetch_tma_descriptor(params.tma_store_dK.get_tma_descriptor());
        cute::prefetch_tma_descriptor(params.tma_store_dV.get_tma_descriptor());
      }
    }
  }

  CUTLASS_DEVICE
  void deterministic_sync(int* range_lock, int bidh, int offset, int q_block_size, int num_heads, int left_range_sync_num, int right_range_sync_num) {
    if (left_range_sync_num == 0 && right_range_sync_num == 0)
      return;

    // Calculate lock index
    int left_range_block_idx = offset / q_block_size;
    int left_range_index = left_range_block_idx * num_heads + bidh;
    int right_range_block_idx = (offset + q_block_size - 1) / q_block_size;

// Acquire the first lock
#pragma unroll 1
    while (atomicCAS(&range_lock[left_range_index * 2], left_range_sync_num, left_range_sync_num) != left_range_sync_num) {
    }

    // If we need a second lock
    if (left_range_block_idx != right_range_block_idx) {
      int right_range_index = right_range_block_idx * num_heads + bidh;

// Try to acquire the second lock
#pragma unroll 1
      while (atomicCAS(&range_lock[right_range_index * 2], right_range_sync_num, right_range_sync_num) != right_range_sync_num) {
      }
    }
  }

  CUTLASS_DEVICE
  void deterministic_arrive(
      int* range_lock,
      int bidh,
      int offset,
      int q_block_size,
      int num_heads,
      int arrive_num,
      bool left_range_arrive_twice,
      bool right_range_arrive_twice) {
    // Calculate lock indices
    int left_range_block_idx = offset / q_block_size;
    int left_range_index = left_range_block_idx * num_heads + bidh;
    int right_range_block_idx = (offset + q_block_size - 1) / q_block_size;
    int right_range_index = right_range_block_idx * num_heads + bidh;

    // Release the second lock
    int add_cnt = right_range_arrive_twice ? 2 : 1;
    int tmp = atomicAdd(&range_lock[right_range_index * 2 + 1], add_cnt);
    // each range_lock needs to arrive twice to make sure conflict batch has been completed
    if (tmp + add_cnt == 2) {
      atomicExch(&range_lock[right_range_index * 2 + 1], 0);
      atomicExch(&range_lock[right_range_index * 2], arrive_num);
    }

    // Release the first lock
    add_cnt = left_range_arrive_twice ? 2 : 1;
    tmp = atomicAdd(&range_lock[left_range_index * 2 + 1], add_cnt);
    if (tmp + add_cnt == 2) {
      atomicExch(&range_lock[left_range_index * 2 + 1], 0);
      atomicExch(&range_lock[left_range_index * 2], arrive_num);
    }
  }

  // Perform a Consumer Epilogue -- TMA store for dK and dV
  // k for outer-loop and q for inner-loop
  template <typename SharedStorage, typename FrgTensorO, typename TiledMma>
  CUTLASS_DEVICE void store_dkv(
      Params const& params,
      FrgTensorO const& tdKrdK,
      FrgTensorO const& tdVrdV,
      SharedStorage& shared_storage,
      TiledMma tiled_mma,
      int thread_idx,
      BlockCoordType const& block_coord) {
    static_assert(!SwapBwdQKLoop, "store_dkv() must be called when SwapBwdQKLoop is false");

    // Get block coordinates for current job (tile)
    int n_block = get<0>(block_coord), bidh = get<1>(block_coord), bidb = get<2>(block_coord);

    int bidh_idx_in_group;
    int bidh_kv = params.qhead_per_khead_divmod.divmod(bidh_idx_in_group, bidh);
    Tensor sdK = cute::as_position_independent_swizzle_tensor(make_tensor(make_smem_ptr(shared_storage.tensors.epilogue.smem_dk.data()), SmemLayoutdKV{}));
    Tensor sdV = cute::as_position_independent_swizzle_tensor(make_tensor(make_smem_ptr(shared_storage.tensors.epilogue.smem_dv.data()), SmemLayoutdKV{}));
    Tensor sdKt = cute::as_position_independent_swizzle_tensor(make_tensor(make_smem_ptr(shared_storage.tensors.epilogue.smem_dk.data()), SmemLayoutdKVt{}));
    Tensor sdVt = cute::as_position_independent_swizzle_tensor(make_tensor(make_smem_ptr(shared_storage.tensors.epilogue.smem_dv.data()), SmemLayoutdKVt{}));
    auto smem_tiled_copy_dKV = make_tiled_copy_C(SmemCopyAtomdKV{}, tiled_mma);
    auto smem_thr_copy_dKV = smem_tiled_copy_dKV.get_thread_slice(thread_idx);

    // Convert the type of tdVrdV and tdKrdK to Element if they are not the same as ElementAccum
    Tensor tdVrdV_out = [&] {
      if constexpr (IsSameType) {
        return tdVrdV;
      } else {
        auto out = make_tensor_like<Element>(tdVrdV);
        flash::convert_type_out(tdVrdV, out);
        return out;
      }
    }();

    Tensor tdKrdK_out = [&] {
      if constexpr (IsSameType) {
        return tdKrdK;
      } else {
        auto out = make_tensor_like<Element>(tdKrdK);
        flash::convert_type_out(tdKrdK, out);
        return out;
      }
    }();

    Tensor taccdKrdK = smem_thr_copy_dKV.retile_S(tdKrdK_out); // ((Atom,AtomNum), MMA_M, MMA_N)
    Tensor taccdVrdV = smem_thr_copy_dKV.retile_S(tdVrdV_out); // ((Atom,AtomNum), MMA_M, MMA_N)

    /* DEBUG */
    // if (blockIdx.x == 0 && threadIdx.x == 128) { print(smem_thr_copy_dKV); print(sdK); printf("\n"); print(sdKt); printf("\n"); }

    Tensor taccdKsdK = smem_thr_copy_dKV.partition_D(cute::conditional_return<!dKV_swapAB>(sdK, sdKt)); // ((Atom,AtomNum),PIPE_M,PIPE_N)
    Tensor taccdVsdV = smem_thr_copy_dKV.partition_D(cute::conditional_return<!dKV_swapAB>(sdV, sdVt)); // ((Atom,AtomNum),PIPE_M,PIPE_N)

    // Make sure all WGs have finished reading K and V
    BarrierManager::sync<NumEpilogueThreads>(resv_barrier::EpilogueBarrier);
    cute::copy(smem_tiled_copy_dKV, taccdVrdV, taccdVsdV);
    cute::copy(smem_tiled_copy_dKV, taccdKrdK, taccdKsdK);

    cutlass::arch::fence_view_async_shared(); // ensure smem writes are visible to TMA
    BarrierManager::arrive<NumEpilogueThreads + cutlass::NumThreadsPerWarp>(resv_barrier::EpilogueBarrier);

    SeqlenInfo_t seqlen_info{bidb, params.q_ranges, params.k_ranges};

    Tensor mdK = params.tma_store_dK.get_tma_tensor(params.shape_dK)(_, _, bidh_kv); // (seqlen_kv, head_dim)
    Tensor mdV = params.tma_store_dV.get_tma_tensor(params.shape_dK)(_, _, bidh_kv); // (seqlen_kv, head_dim)
    Tensor gdK = local_tile(domain_offset(make_coord(seqlen_info.offset_k, _0{}), mdK), select<1, 2>(TileShape_MNK{}), make_coord(n_block, _0{})); // (N, K)
    Tensor gdV = local_tile(domain_offset(make_coord(seqlen_info.offset_k, _0{}), mdV), select<1, 2>(TileShape_MNK{}), make_coord(n_block, _0{})); // (N, K)

    auto block_tma_dK = params.tma_store_dK.get_slice(_0{});
    auto block_tma_dV = params.tma_store_dV.get_slice(_0{});
    Tensor tdKgdK = block_tma_dK.partition_D(gdK); // (TMA, TMA_N, TMA_K)
    Tensor tdKsdK = block_tma_dK.partition_S(sdK); // (TMA, TMA_N, TMA_K)
    Tensor tdVgdV = block_tma_dV.partition_D(gdV); // (TMA, TMA_N, TMA_K)
    Tensor tdVsdV = block_tma_dV.partition_S(sdV); // (TMA, TMA_N, TMA_K)

    int warp_idx_sync = warp_uniform(thread_idx / cutlass::NumThreadsPerWarp);
    if (warp_idx_sync == NumEpilogueThreads / cutlass::NumThreadsPerWarp - 1) {
      if constexpr (Deterministic) {
        if (cute::elect_one_sync()) {
          int left_range_conflict_msg = get<3>(block_coord);
          int right_range_conflict_msg = get<4>(block_coord);
          int arrive_num = get<5>(block_coord);
          int qheads_per_kheads = params.qhead_per_khead_divmod;
          // batch i use [i * qheads_per_kheads + 1 , (i + 1) * qheads_per_kheads] for add rank of same khead
          // conflict_msg >> 1 is bidb + 1 of conflict bidb when conflict with previous batch
          // if not conflict, conflict_msg >> 1 is 0
          // the first gqa headq should wait for conflict batches
          // the others gqa headq should wait for previous gqa headq
          int sync_num1 = bidh_idx_in_group ? arrive_num * qheads_per_kheads + bidh_idx_in_group : (left_range_conflict_msg >> 1) * qheads_per_kheads;
          int sync_num2 = bidh_idx_in_group ? arrive_num * qheads_per_kheads + bidh_idx_in_group : (right_range_conflict_msg >> 1) * qheads_per_kheads;
          deterministic_sync(params.determin_range_locks, bidh_kv, seqlen_info.offset_k + n_block * kBlockN, kBlockN, params.nheads, sync_num1, sync_num2);
        }
      }
      BarrierManager::sync<NumEpilogueThreads + cutlass::NumThreadsPerWarp>(resv_barrier::EpilogueBarrier);
      if (cute::elect_one_sync()) {
        cute::copy(params.tma_store_dV, tdVsdV, tdVgdV);
        cute::copy(params.tma_store_dK, tdKsdK, tdKgdK);
        tma_store_arrive();
      }
    }

    tma_store_wait<0>();

    if constexpr (Deterministic) {
      if (warp_idx_sync == NumEpilogueThreads / cutlass::NumThreadsPerWarp - 1 && cute::elect_one_sync()) {
        int left_range_conflict_msg = get<3>(block_coord);
        int right_range_conflict_msg = get<4>(block_coord);
        int qheads_per_kheads = params.qhead_per_khead_divmod;
        int arrive_num = get<5>(block_coord);
        arrive_num = arrive_num * qheads_per_kheads + bidh_idx_in_group + 1;
        deterministic_arrive(
            params.determin_range_locks,
            bidh_kv,
            seqlen_info.offset_k + n_block * kBlockN,
            kBlockN,
            params.nheads,
            arrive_num,
            left_range_conflict_msg & 1,
            right_range_conflict_msg & 1);
      }
    }
  }

  // Perform a Consumer Epilogue -- TMA store for dQ
  // q for outer-loop and k for inner-loop
  template <typename SharedStorage, typename FrgTensorO, typename TiledMma>
  CUTLASS_DEVICE void store_dq(
      Params const& params,
      FrgTensorO const& tdQrdQ,
      SharedStorage& shared_storage,
      TiledMma tiled_mma,
      int thread_idx,
      BlockCoordType const& block_coord) {
    static_assert(SwapBwdQKLoop, "store_dq() must be called when SwapBwdQKLoop is true");
    static_assert(!Deterministic, "Deterministic mode is not supported yet");

    // Get block coordinates for current job (tile)
    int m_block = get<0>(block_coord), bidh = get<1>(block_coord), bidb = get<2>(block_coord);

    int bidh_idx_in_group;
    int bidh_kv = params.qhead_per_khead_divmod.divmod(bidh_idx_in_group, bidh);
    Tensor sdQ = cute::as_position_independent_swizzle_tensor(make_tensor(make_smem_ptr(shared_storage.tensors.epilogue.smem_dq.data()), SmemLayoutdQ{}));
    Tensor sdQt = cute::as_position_independent_swizzle_tensor(make_tensor(make_smem_ptr(shared_storage.tensors.epilogue.smem_dq.data()), SmemLayoutdQt{}));

    auto smem_tiled_copy_dQ = make_tiled_copy_C(SmemCopyAtomdQ{}, tiled_mma);
    auto smem_thr_copy_dQ = smem_tiled_copy_dQ.get_thread_slice(thread_idx);

    // Convert the type of tdQrdQ to Element if they are not the same as ElementAccum
    Tensor tdQrdQ_out = [&] {
      if constexpr (IsSameType) {
        return tdQrdQ;
      } else {
        auto out = make_tensor_like<Element>(tdQrdQ);
        flash::convert_type_out(tdQrdQ, out);
        return out;
      }
    }();

    Tensor taccdQrdQ = smem_thr_copy_dQ.retile_S(tdQrdQ_out); // ((Atom,AtomNum), MMA_M, MMA_N)

    /* DEBUG */
    // if (blockIdx.x == 0 && threadIdx.x == 128) { print(smem_thr_copy_dQ); print(sdQ); printf("\n"); print(sdQt); printf("\n"); }

    Tensor taccdQsdQ = smem_thr_copy_dQ.partition_D(cute::conditional_return<!dQ_swapAB>(sdQ, sdQt)); // ((Atom,AtomNum),PIPE_M,PIPE_N)

    // Make sure all WGs have finished reading Q
    BarrierManager::sync<NumEpilogueThreads>(resv_barrier::EpilogueBarrier);
    cute::copy(smem_tiled_copy_dQ, taccdQrdQ, taccdQsdQ);

    cutlass::arch::fence_view_async_shared(); // ensure smem writes are visible to TMA
    BarrierManager::arrive<NumEpilogueThreads + cutlass::NumThreadsPerWarp>(resv_barrier::EpilogueBarrier);
    SeqlenInfo_t seqlen_info{bidb, params.q_ranges, params.k_ranges};

    Tensor mdQ = params.tma_store_dQ.get_tma_tensor(params.shape_dK)(_, _, bidh); // (seqlen_q, head_dim)
    Tensor gdQ = local_tile(domain_offset(make_coord(seqlen_info.offset_q, _0{}), mdQ), select<0, 2>(TileShape_MNK{}), make_coord(m_block, _0{})); // (M, K)

    auto block_tma_dQ = params.tma_store_dQ.get_slice(_0{});
    Tensor tdQgdQ = block_tma_dQ.partition_D(gdQ); // (TMA, TMA_M, TMA_K)
    Tensor tdQsdQ = block_tma_dQ.partition_S(sdQ); // (TMA, TMA_M, TMA_K)

    int warp_idx_sync = warp_uniform(thread_idx / cutlass::NumThreadsPerWarp);
    if (warp_idx_sync == NumEpilogueThreads / cutlass::NumThreadsPerWarp - 1) {
      BarrierManager::sync<NumEpilogueThreads + cutlass::NumThreadsPerWarp>(resv_barrier::EpilogueBarrier);
      if (cute::elect_one_sync()) {
        cute::copy(params.tma_store_dQ, tdQsdQ, tdQgdQ);
        tma_store_arrive();
      }
    }

    tma_store_wait<0>();
  }

  CUTLASS_DEVICE void store_tail() {
    // if constexpr (Use_TMA) { tma_store_wait<0>(); }
  }

  // Write 0 to dK and dV
  // k for outer-loop and q for inner-loop
  CUTLASS_DEVICE void store_zero_dkv(Params const& params, int thread_idx, BlockCoordType const& block_coord) {
    if constexpr (Deterministic) {
      int warp_idx_sync = warp_uniform(thread_idx / cutlass::NumThreadsPerWarp);
      if (warp_idx_sync == NumEpilogueThreads / cutlass::NumThreadsPerWarp - 1 && cute::elect_one_sync()) {
        // Get block coordinates for current job(tile)
        int n_block = get<0>(block_coord);
        int bidh = get<1>(block_coord);
        int bidb = get<2>(block_coord);
        int left_range_conflict_msg = get<3>(block_coord);
        int right_range_conflict_msg = get<4>(block_coord);
        int arrive_num = get<5>(block_coord);
        int bidh_idx_in_group;
        int bidh_kv = params.qhead_per_khead_divmod.divmod(bidh_idx_in_group, bidh);
        SeqlenInfo_t seqlen_info{bidb, params.q_ranges, params.k_ranges};
        int offset_k = seqlen_info.offset_k;
        int qheads_per_kheads = params.qhead_per_khead_divmod;
        // batch i use [i * qheads_per_kheads + 1 , (i + 1) * qheads_per_kheads] for add rank of same khead
        // conflict_msg >> 1 is bidb + 1 of conflict bidb when conflict with previous batch
        // if not conflict, conflict_msg >> 1 is 0
        // the first gqa headq should wait for conflict batches
        // the others gqa headq should wait for previous gqa headq
        int sync_num1 = bidh_idx_in_group ? arrive_num * qheads_per_kheads + bidh_idx_in_group : (left_range_conflict_msg >> 1) * qheads_per_kheads;
        int sync_num2 = bidh_idx_in_group ? arrive_num * qheads_per_kheads + bidh_idx_in_group : (right_range_conflict_msg >> 1) * qheads_per_kheads;
        deterministic_sync(params.determin_range_locks, bidh_kv, offset_k + n_block * kBlockN, kBlockN, params.nheads, sync_num1, sync_num2);
        arrive_num = arrive_num * qheads_per_kheads + bidh_idx_in_group + 1;
        deterministic_arrive(
            params.determin_range_locks,
            bidh_kv,
            offset_k + n_block * kBlockN,
            kBlockN,
            params.nheads,
            arrive_num,
            left_range_conflict_msg & 1,
            right_range_conflict_msg & 1);
      }
    }
  }

  // Write 0 to dQ
  // q for outer-loop and k for inner-loop
  CUTLASS_DEVICE void store_zero_dq(Params const& params, int thread_idx, BlockCoordType const& block_coord) {
    if constexpr (Deterministic) {
      static_assert(!Deterministic, "Deterministic mode is not supported yet");
    }
  }
};

} // namespace flash
